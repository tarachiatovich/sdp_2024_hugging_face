{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230aa870-f2a4-438d-bef7-5682e455ae71",
   "metadata": {},
   "source": [
    "This is a Hugging Face fail! Q: Why am I sharing this failure with you? A: It's a good demonstration of the limitations of\n",
    "Hugging Face. This simple GPT model not only had trouble giving me the right answers to simple questions, it also gave me\n",
    "inconsistent answers as I ran and reran the code after \"tweaking\" things to try to supress/address some warning messages.\n",
    "Part of the problem may be that it's an older GPT model...but that to me doesn't explain the different answers (some right,\n",
    "some wrong, some different wrong answers to the same question)...and I'm afraid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3889d4-9a96-451f-8657-cf84154a941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any packages you have not yet installed. Again, you can run the next code chunk as a way of checking whether you have \n",
    "# them installed. You will get an error saying the package is not found if it has not been installed. Remove the # from the beginning\n",
    "# of any line with a package you need to install.\n",
    "# Note: transformers was used elsewhere in the Hugging Face session and should be installed. You should not need to run the following line!\n",
    "# %pip install transformers\n",
    "# %pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be622d55-0254-49bd-8259-4a6d86ba4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face fail!\n",
    "# Source: https://thepythoncode.com/article/conversational-ai-chatbot-with-huggingface-transformers-in-python\n",
    "# FYI: Many parts of the code below are copied and pasted directly from the above. My changes/additions had to\n",
    "# do with supressing or addressing warning messages.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from transformers.utils import logging # Used to supress an annoying warning message from our GPT model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5597620-e239-4ddf-b239-ff068a70b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the code to turn off annoying warning message\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744188e-7e91-4b3b-8511-9beecf23e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b769fd-831d-4f07-a834-d000d757a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last output tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
