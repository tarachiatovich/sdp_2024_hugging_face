{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230aa870-f2a4-438d-bef7-5682e455ae71",
   "metadata": {},
   "source": [
    "This is a Hugging Face fail! Q: Why am I sharing this failure with you? A: It's a good demonstration of the limitations of\n",
    "Hugging Face. This simple GPT model not only had trouble giving me the right answers to simple questions, it also gave me\n",
    "inconsistent answers as I ran and reran the code after \"tweaking\" things to try to supress/address some warning messages.\n",
    "Part of the problem may be that it's an older GPT model...but that to me doesn't explain the different answers (some right,\n",
    "some wrong, some different wrong answers to the same question)...and I'm afraid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a3889d4-9a96-451f-8657-cf84154a941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any packages you have not yet installed. Again, you can run the next code chunk as a way of checking whether you have \n",
    "# them installed. You will get an error saying the package is not found if it has not been installed. Remove the # from the beginning\n",
    "# of any line with a package you need to install.\n",
    "# Note: transformers was used elsewhere in the Hugging Face session and should be installed. You should not need to run the following line!\n",
    "# %pip install transformers\n",
    "# %pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be622d55-0254-49bd-8259-4a6d86ba4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face fail!\n",
    "# Source: https://thepythoncode.com/article/conversational-ai-chatbot-with-huggingface-transformers-in-python\n",
    "# FYI: Many parts of the code below are copied and pasted directly from the above. My changes/additions had to\n",
    "# do with supressing or addressing warning messages.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from transformers.utils import logging # Used to supress an annoying warning message from our GPT model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5597620-e239-4ddf-b239-ff068a70b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the code to turn off annoying warning message\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744188e-7e91-4b3b-8511-9beecf23e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\TaraChiatovich\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium\\snapshots\\7b40bb0f92c45fefa957d088000d8648e5c7fa33\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\TaraChiatovich\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium\\snapshots\\7b40bb0f92c45fefa957d088000d8648e5c7fa33\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\TaraChiatovich\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium\\snapshots\\7b40bb0f92c45fefa957d088000d8648e5c7fa33\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\TaraChiatovich\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium\\snapshots\\7b40bb0f92c45fefa957d088000d8648e5c7fa33\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"microsoft/DialoGPT-medium\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"conversational\": {\n",
      "      \"max_length\": 1000\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\TaraChiatovich\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium\\snapshots\\7b40bb0f92c45fefa957d088000d8648e5c7fa33\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initiate the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b769fd-831d-4f07-a834-d000d757a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although I got this to run before (see the screenshot in my slides), I could not get it to run after putting the code in \n",
    "# a new notebook. This is yet another fail! It may have to do with me installing a different version of the packages used,\n",
    "# but it adds to the mystery of the immense failure I have had with this simpler open-source version of GPT!\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last output tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
